# ğŸ™ï¸ Smart Estate Recommender & Valuator

A minimal, reproducible **real estate data pipeline** that scrapes Avito listings and processes them through a modern streaming architecture.

---

## ğŸ¯ Overview

**Pipeline Flow:**
Avito Scraper â†’ **Kafka** â†’ **Spark Structured Streaming** â†’ **Apache Iceberg** (on **MinIO**) â†’ Orchestrated by **Airflow**

This repository provides a complete Docker Compose setup with:

* **Kafka (KRaft mode)** + **Kafka UI**
* **MinIO** object storage (S3-compatible)
* **Iceberg REST** catalog server
* **Spark** worker with Iceberg + AWS SDK integration
* **Avito Scraper** container (Python requests/BeautifulSoup4)
* **Airflow** (web server + scheduler + PostgreSQL) with automated DAG:
  * Ensures Kafka topic exists
  * Maintains streaming Spark job
  * Executes scraper every 5 minutes

---

## ğŸ“‹ Prerequisites

* **Docker** & **Docker Compose** installed
* **~4 GB RAM** available
* **Required ports** free:
  * `8088` â€” Airflow web UI
  * `8090` â€” Kafka UI
  * `9000` â€” MinIO API
  * `9001` â€” MinIO console
  * `8181` â€” Iceberg REST server
  * `8888` â€” JupyterLab (EDA Silver)

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/Bjibjihamza/smart-estate-recommender-valuator.git
cd smart-estate-recommender-valuator
```

### 2ï¸âƒ£ Configure Environment

Create a `.env` file in the repository root:

```env
# MinIO Configuration
MINIO_ROOT_USER=admin
MINIO_ROOT_PASSWORD=admin123
LAKE_BUCKET=lake

# Kafka Configuration
KAFKA_OUTSIDE_PORT=9094
KAFKA_UI_PORT=8090

# Airflow Configuration
AIRFLOW_USER=admin
AIRFLOW_PASSWORD=admin

# Spark Notebook
JUPYTER_TOKEN=serv

# Optional Ports
MINIO_API_PORT=9000
MINIO_CONSOLE_PORT=9001
```

### 3ï¸âƒ£ Build and Start Services

```bash
docker compose up -d --build
```

Wait ~2â€“3 minutes for all services to become healthy.

Verify:

```bash
docker ps
```

Expected running containers:
âœ… kafka
âœ… minio
âœ… iceberg-rest
âœ… spark-iceberg
âœ… spark-notebook
âœ… airflow-db
âœ… airflow-web
âœ… airflow-scheduler
âœ… avito-scraper

---

## âš™ï¸ Initialize Airflow Admin User (First Time Only)

```bash
docker exec -it airflow-web airflow users create \
  --username admin --firstname Admin --lastname User \
  --role Admin --email admin@example.com --password admin
```

If "User already exists", skip this step.

---

## ğŸ§Š Create Iceberg Tables & Namespaces

We use Python scripts to create the Iceberg namespaces and tables programmatically. This approach is cleaner and more maintainable than manual SQL commands.

### ğŸ“ Table Structure

```
rest/
â”œâ”€â”€ raw/
â”‚   â””â”€â”€ avito (id, payload, ingest_ts)  â† Raw JSON data from Kafka
â””â”€â”€ silver/
    â””â”€â”€ avito (31 columns)              â† Cleaned & structured data
```

### a. Create RAW Layer

The **raw** layer stores the original JSON payload from Kafka with minimal processing.

**Schema:**
- `id` (STRING) â€” Listing ID
- `payload` (STRING) â€” Raw JSON document
- `ingest_ts` (TIMESTAMP) â€” Ingestion timestamp
- **Partitioned by:** `days(ingest_ts)`

**Run:**

```bash
docker exec -it spark-iceberg bash -lc "
/opt/spark/bin/spark-submit \
  --master local[*] \
  /opt/work/src/database/raw.py
"
```

**Expected output:**
```
============================================================
Creating Raw Layer in Iceberg
============================================================
[INFO] Creating namespace 'raw' if not exists...
[INFO] Creating table 'raw.avito' with schema...
[SUCCESS] Raw namespace and table created successfully!
```

### b. Create SILVER Layer

The **silver** layer contains cleaned, structured, and enriched data ready for analytics.

**Schema (31 columns):**
- Core fields: `id`, `url`, `title`, `price`, `description`
- Seller info: `seller_name`, `seller_type`
- Location: `city`, `neighborhood`, `site`
- Metadata: `offre`, `type`, `published_date`, `ingest_ts`
- Arrays: `image_urls`, `equipments`
- Property attributes: `Surface habitable`, `Chambres`, `Ã‰tage`, etc.
- **Partitioned by:** `days(ingest_ts)`

**Run:**

```bash
docker exec -it spark-iceberg bash -lc "
/opt/spark/bin/spark-submit \
  --master local[*] \
  /opt/work/src/database/silver.py
"
```

**Expected output:**
```
============================================================
Creating Silver Layer in Iceberg
============================================================
[INFO] Creating namespace 'silver' if not exists...
[INFO] Creating table 'silver.avito' with schema...
[SUCCESS] Silver namespace and table created successfully!
+---------+---------+-----------+
|namespace|tableName|isTemporary|
+---------+---------+-----------+
|silver   |avito    |false      |
+---------+---------+-----------+
```

### c. Verify Tables Creation

```bash
docker exec -it spark-iceberg /opt/spark/bin/spark-sql \
  --conf spark.sql.catalog.rest=org.apache.iceberg.spark.SparkCatalog \
  --conf spark.sql.catalog.rest.uri=http://iceberg-rest:8181 \
  --conf spark.sql.catalog.rest.warehouse=s3://lake/warehouse \
  --conf spark.sql.catalog.rest.io-impl=org.apache.iceberg.aws.s3.S3FileIO \
  --conf spark.sql.catalog.rest.s3.endpoint=http://minio:9000 \
  --conf spark.sql.catalog.rest.s3.access-key-id=admin \
  --conf spark.sql.catalog.rest.s3.secret-access-key=admin123 \
  -e "SHOW NAMESPACES IN rest; SHOW TABLES IN rest.raw; SHOW TABLES IN rest.silver;"
```

---

## ğŸ” Start Kafka â†’ Iceberg Streaming Sink

This streaming job continuously reads from Kafka and writes to the **raw.avito** table.

```bash
docker exec -d spark-iceberg bash -c "nohup /opt/spark/bin/spark-submit --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions --conf spark.sql.catalog.rest=org.apache.iceberg.spark.SparkCatalog --conf spark.sql.catalog.rest.catalog-impl=org.apache.iceberg.rest.RESTCatalog --conf spark.sql.catalog.rest.uri=http://iceberg-rest:8181 --conf spark.sql.catalog.rest.warehouse=s3://lake/warehouse --conf spark.sql.catalog.rest.io-impl=org.apache.iceberg.aws.s3.S3FileIO --conf spark.sql.catalog.rest.s3.endpoint=http://minio:9000 --conf spark.sql.catalog.rest.s3.path-style-access=true --conf spark.sql.catalog.rest.s3.access-key-id=admin --conf spark.sql.catalog.rest.s3.secret-access-key=admin123 --conf spark.sql.defaultCatalog=rest /opt/work/src/Pipeline/load/iceberg_kafka_sink.py --kafka-bootstrap kafka:9092 --topic realestate.avito.raw --table rest.raw.avito --checkpoint file:///opt/work/checkpoints/avito_raw --starting-offsets latest --trigger '15 seconds' > /opt/work/logs/avito_sink.log 2>&1 &"

```

Check it's running:

```bash
docker exec -it spark-iceberg ps aux | grep spark-submit
```

View logs:

```bash
docker exec -it spark-iceberg tail -f /opt/work/logs/avito_sink.log
```

---

## ğŸŒ Access the UIs

| Service              | URL                                                                          | Credentials        |
| -------------------- | ---------------------------------------------------------------------------- | ------------------ |
| **Airflow**          | [http://localhost:8088](http://localhost:8088)                               | `admin / admin`    |
| **Kafka UI**         | [http://localhost:8090](http://localhost:8090)                               | -                  |
| **MinIO Console**    | [http://localhost:9001](http://localhost:9001)                               | `admin / admin123` |
| **JupyterLab (EDA)** | [http://localhost:8888/lab?token=serv](http://localhost:8888/lab?token=serv) | `Token: serv`      |

---

## âœ… Verification & Testing

### Check Kafka Messages

```bash
docker exec -it kafka kafka-console-consumer \
  --bootstrap-server kafka:9092 \
  --topic realestate.avito.raw \
  --from-beginning --max-messages 1
```

### Check RAW Table Data

```bash
docker exec -it spark-iceberg /opt/spark/bin/spark-sql --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions --conf spark.sql.catalog.rest=org.apache.iceberg.spark.SparkCatalog --conf spark.sql.catalog.rest.catalog-impl=org.apache.iceberg.rest.RESTCatalog --conf spark.sql.catalog.rest.uri=http://iceberg-rest:8181 --conf spark.sql.catalog.rest.warehouse=s3://lake/warehouse --conf spark.sql.catalog.rest.io-impl=org.apache.iceberg.aws.s3.S3FileIO --conf spark.sql.catalog.rest.s3.endpoint=http://minio:9000 --conf spark.sql.catalog.rest.s3.path-style-access=true --conf spark.sql.catalog.rest.s3.access-key-id=admin --conf spark.sql.catalog.rest.s3.secret-access-key=admin123 --conf spark.sql.defaultCatalog=rest -S -e "SHOW NAMESPACES; SHOW TABLES IN rest.raw; SELECT COUNT(*) AS raw_rows FROM rest.raw.avito; SELECT COUNT(*) AS silver_rows FROM rest.silver.avito;"
```

### Check SILVER Table (After Transformation)

```bash
docker exec -it spark-iceberg /opt/spark/bin/spark-sql \
  --conf spark.sql.catalog.rest=org.apache.iceberg.spark.SparkCatalog \
  --conf spark.sql.catalog.rest.uri=http://iceberg-rest:8181 \
  --conf spark.sql.catalog.rest.warehouse=s3://lake/warehouse \
  --conf spark.sql.catalog.rest.io-impl=org.apache.iceberg.aws.s3.S3FileIO \
  --conf spark.sql.catalog.rest.s3.endpoint=http://minio:9000 \
  --conf spark.sql.catalog.rest.s3.access-key-id=admin \
  --conf spark.sql.catalog.rest.s3.secret-access-key=admin123 \
  -e "SELECT id, title, price, city, offre FROM rest.silver.avito LIMIT 10;"
```

---

## ğŸ§  Exploratory Data Analysis (EDA â€” Silver Dataset)

A **JupyterLab environment** is integrated for interactive data exploration and preparation before building **Silver** tables.

### ğŸ“˜ Notebook Environment

* **Container:** `spark-notebook`
* **URL:** [http://localhost:8888/lab?token=serv](http://localhost:8888/lab?token=serv)
* **Workspace:** `/opt/work/notebooks/`

### âš™ï¸ Configuration

The JupyterLab image includes:

* **Apache Spark 3.5.1**
* **Iceberg runtime 1.6.0**
* **Python 3.8 + PySpark, Pandas, Boto3**
* Predefined token via `.env`:

  ```env
  JUPYTER_TOKEN=serv
  ```

### ğŸ§ª Example: Connect to Iceberg

```python
from pyspark.sql import SparkSession

spark = (
    SparkSession.builder.appName("Iceberg via REST")
    .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")
    .config("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog")
    .config("spark.sql.catalog.local.catalog-impl", "org.apache.iceberg.rest.RESTCatalog")
    .config("spark.sql.catalog.local.uri", "http://iceberg-rest:8181")
    .config("spark.sql.catalog.local.io-impl", "org.apache.iceberg.aws.s3.S3FileIO")
    .config("spark.sql.catalog.local.warehouse", "s3://lake/warehouse")
    .config("spark.sql.catalog.local.s3.endpoint", "http://minio:9000")
    .config("spark.sql.catalog.local.s3.path-style-access", "true")
    .config("spark.sql.catalog.local.s3.access-key-id", "admin")
    .config("spark.sql.catalog.local.s3.secret-access-key", "admin123")
    .getOrCreate()
)

# Load raw data
raw_df = spark.table("local.raw.avito")
raw_df.show(5)

# Load silver data
silver_df = spark.table("local.silver.avito")
silver_df.printSchema()
```

Access shell if needed:

```bash
docker exec -it spark-notebook bash
```

---

## ğŸ“Š Pipeline DAG

**File:** `dags/avito_scraper.py`
**Schedule:** Every 5 minutes

### DAG Tasks

1. **Choose Mode** â€” Alternates between `louer` (rent) and `acheter` (buy)
2. **Run Scraper** â€” Extracts & pushes new Avito listings to Kafka
3. **Done** â€” Marks completion

The DAG automatically alternates between scraping rental and sale listings on each run.

---

## ğŸ› ï¸ Troubleshooting

### Restart Streaming Sink

If the streaming job stops:

```bash
# Kill existing process
docker exec -it spark-iceberg pkill -f iceberg_kafka_sink

# Restart (use command from section above)
docker exec -d spark-iceberg bash -c "nohup /opt/spark/bin/spark-submit ..."
```

### Check Logs

```bash
# Streaming sink logs
docker exec -it spark-iceberg tail -f /opt/work/logs/avito_sink.log

# Airflow logs
docker logs airflow-scheduler -f

# Spark logs
docker logs spark-iceberg -f
```

### Recreate Tables

If you need to drop and recreate tables:

```bash
docker exec -it spark-iceberg /opt/spark/bin/spark-sql \
  --conf spark.sql.catalog.rest.uri=http://iceberg-rest:8181 \
  -e "DROP TABLE IF EXISTS rest.raw.avito; DROP TABLE IF EXISTS rest.silver.avito;"

# Then run the creation scripts again
```

---

## ğŸ“ Repository Structure

```
smart-estate-recommender-valuator/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ avito_scraper.py           # Airflow DAG for orchestration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ database/                  # Table creation scripts
â”‚   â”‚   â”œâ”€â”€ raw.py                 # Creates rest.raw.avito
â”‚   â”‚   â””â”€â”€ silver.py              # Creates rest.silver.avito
â”‚   â”œâ”€â”€ notebooks/                 # Jupyter EDA workspace
â”‚   â””â”€â”€ Pipeline/
â”‚       â”œâ”€â”€ extract/
â”‚       â”‚   â””â”€â”€ avito_scraper.py   # Web scraper logic
â”‚       â”œâ”€â”€ producer/
â”‚       â”‚   â””â”€â”€ avito_producer.py  # Kafka producer
â”‚       â””â”€â”€ load/
â”‚           â””â”€â”€ iceberg_kafka_sink.py  # Streaming sink
â”œâ”€â”€ Dockerfile                     # Scraper container
â”œâ”€â”€ Dockerfile.spark               # Spark + Iceberg container
â”œâ”€â”€ docker-compose.yml             # Full stack definition
â”œâ”€â”€ .env                           # Environment variables
â””â”€â”€ README.md
```

---

## ğŸ“ Data Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Avito.ma     â”‚
â”‚ (Web Source) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP Requests
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Python Scraper   â”‚  â† Airflow scheduled (every 5 min)
â”‚ (BeautifulSoup4) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ JSON Messages
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kafka Topic      â”‚  â† realestate.avito.raw
â”‚ (KRaft Mode)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Streaming Read
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Spark Structured â”‚  â† Continuous micro-batches (15s)
â”‚ Streaming        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Write
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Iceberg REST     â”‚  â† rest.raw.avito (partitioned)
â”‚ Catalog + MinIO  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Transform
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Silver Layer     â”‚  â† rest.silver.avito (cleaned)
â”‚ (31 columns)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¤ Contributing

Contributions and enhancements are welcome!
Create feature branches (e.g., `feature/eda-silver`) and open pull requests.

---

## ğŸ“„ License

[MIT License] â€” 2025 Â© Hamza Bjibji
